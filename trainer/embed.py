import tensorflow as tf

import modules as m

class TrainableEmbedding(object):

    def __init__(self, emb_mat=None):
        self.emb_mat = tf.get_variable("embed_mat", initializer=tf.constant(
            emb_mat, dtype=tf.float32), trainable=True)

    def get_embedding(self, x):
        emb = tf.nn.embedding_lookup(self.emb_mat, x)

        oov_mask = tf.cast(tf.reduce_sum(emb, axis=-1), tf.bool)
        emb = tf.where(oov_mask, tf.stop_graident(emb), emb)

        return emb


def char_embedding(x, emb_mat, 
            cell, seqlen, hidden, keep_prob, is_train, reuse=False, birnn=True): # The character-level embeddings are generated by taking the final hidden states of a bi-directional recurrent neural network (RNN) applied to embeddings of characters in the token
    embed = tf.nn.embedding_lookup(emb_mat, x)
    if birnn:
        cell_fw = m._create_cell(hidden, keep_prob, cell, "fw_cell", reuse=reuse, is_train=is_train)
        cell_bw = m._create_cell(hidden, keep_prob, cell, "bw_cell", reuse=reuse, is_train=is_train)
        _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, embed, seqlen, dtype=tf.float32)
        outputs = tf.concat([state_fw, state_bw], axis=1)
    else:
        cell = m._create_cell(hidden, keep_prob, cell, "cell", reuse=reuse, is_train=is_train)
        _, state = tf.nn.dynamic_rnn(cell, embed, seqlen, dtype=tf.float32)
        outputs = state
    return outputs
