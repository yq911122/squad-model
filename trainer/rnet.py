import tensorflow as tf

import modules as m
import embed as e

class RNET(object):
    """R-net: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf. some code is borrowed from https://github.com/HKUST-KnowComp/R-Net"""

    def __init__(self, config, batch, word_mat=None, char_mat=None, is_train=True):

        self.is_train = tf.get_variable(
            "is_train", shape=[], dtype=tf.bool, trainable=False)
        self.word_mat = tf.get_variable("word_mat", initializer=tf.constant(
            word_mat, dtype=tf.float32), trainable=False)
        self.char_mat = tf.get_variable(
            "char_mat", initializer=tf.constant(char_mat, dtype=tf.float32))
        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,
                                           initializer=tf.constant_initializer(0), trainable=False)
        self.inference(config, batch)

        if is_train:
            self.lr = tf.get_variable(
                "lr", shape=[], dtype=tf.float32, trainable=False)
            self.opt = tf.train.AdadeltaOptimizer(
                learning_rate=self.lr, epsilon=1e-6)
            grads = self.opt.compute_gradients(self.loss)
            gradients, variables = zip(*grads)
            capped_grads, _ = tf.clip_by_global_norm(
                gradients, config.grad_clip)
            self.train_op = self.opt.apply_gradients(
                zip(capped_grads, variables), global_step=self.global_step)     

    def inference(self, config, batch):

        K, H, N, CH, CL = config.num_encoding_layers, config.hidden, config.batch_size, config.char_hidden, config.char_limit

        cw, qw, ch, qh, y1, y2, self.qa_id = batch.get_next()
        q_mask, q_len = m.get_mask_and_len(qw)
        c_mask, c_len = m.get_mask_and_len(cw)

        c_maxlen = tf.reduce_max(c_len)
        q_maxlen = tf.reduce_max(q_len)
        cw = tf.slice(cw, [0, 0], [N, c_maxlen])
        qw = tf.slice(qw, [0, 0], [N, q_maxlen])
        c_mask = tf.slice(c_mask, [0, 0], [N, c_maxlen])
        q_mask = tf.slice(q_mask, [0, 0], [N, q_maxlen])
        ch = tf.slice(ch, [0, 0, 0], [N, c_maxlen, CL])
        qh = tf.slice(qh, [0, 0, 0], [N, q_maxlen, CL])
        y1 = tf.slice(y1, [0, 0], [N, c_maxlen])
        y2 = tf.slice(y2, [0, 0], [N, c_maxlen])

        with tf.variable_scope("embedding"):
            with tf.variable_scope("word"):
                qw_emb = tf.nn.embedding_lookup(self.word_mat, qw)
                cw_emb = tf.nn.embedding_lookup(self.word_mat, cw)

            with tf.variable_scope("char"): # The character-level embeddings are generated by taking the final hidden states of a bi-directional recurrent neural network (RNN) applied to embeddings of characters in the token
                ch = tf.reshape(ch, [-1, CL])
                qh = tf.reshape(qh, [-1, CL])
                _, ch_len = m.get_mask_and_len(ch)
                _, qh_len = m.get_mask_and_len(qh)
                qc_emb = e.char_embedding(qh, self.char_mat, "gru", qh_len, CH, config.keep_prob, self.is_train)
                cc_emb = e.char_embedding(ch, self.char_mat, "gru", ch_len, CH, config.keep_prob, self.is_train, reuse=True)
                qc_emb = tf.reshape(qc_emb, [N, -1, 2 * CH])
                cc_emb = tf.reshape(cc_emb, [N, -1, 2 * CH])
            q_emb = tf.concat([qw_emb, qc_emb], axis=2)
            c_emb = tf.concat([cw_emb, cc_emb], axis=2)

        q, c = q_emb, c_emb

        with tf.variable_scope("encoding"):
            with tf.variable_scope('c'):
                model = m.DropoutBiRNN(num_layers=1, hidden=H, cell_type=config.cell_type, dropout_keep_prob=config.keep_prob, is_train=self.is_train)
                c = model(inputs=c, seqlen=c_len)
            with tf.variable_scope('q'):
                model = m.DropoutBiRNN(num_layers=1, hidden=H, cell_type=config.cell_type, dropout_keep_prob=config.keep_prob, is_train=self.is_train)
                q = model(inputs=q, seqlen=q_len)

        with tf.variable_scope("match"):
            model = m.AttentionBiRNN(num_layers=1, hidden=H, cell_type=config.cell_type, dropout_keep_prob=config.keep_prob, is_train=self.is_train)
            v = model(query=q, mask=q_mask, memory=c, seqlen=q_len, gate=True)

        with tf.variable_scope("self_match"):
            model = m.AttentionBiRNN(num_layers=1, hidden=H, cell_type=config.cell_type, dropout_keep_prob=config.keep_prob, is_train=self.is_train)
            h = model(query=v, mask=c_mask, memory=v, seqlen=c_len, gate=False)

        with tf.variable_scope("pointer"):
            init_model = m.AttentionPool(hidden=H, dropout_keep_prob=config.ptr_keep_prob, is_train=self.is_train)
            init_state, _ = init_model(query=q, mask=q_mask)
            model = m.AttentionPointerNet(hidden=H, cell_type=config.cell_type, dropout_keep_prob=config.ptr_keep_prob, is_train=self.is_train)
            self.st_logits, self.end_logits = model(init_state=init_state, memory=h, mask=c_mask)

        with tf.variable_scope("predict"):
            self.loss, (self.yp1, self.yp2) = m.pointer_boundary_loss_and_prediction(y1, y2, self.st_logits, self.end_logits)

    def get_loss(self):
        return self.loss

    def get_global_step(self):
        return self.global_step

